{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploração e Tratamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2 nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração das questões em PDF\n",
    "Aqui serão salvos 2 CSVs, um com o enunciado e alternativas juntos e outro com cada um separado.\n",
    "\n",
    "No segundo caso pode haver algumas aberrações, a maioria das alternativas termina em apenas uma linha, mas uma mesma alternativa pode ter mais de uma linha, o que não foi levado em consideração na lógica do código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_from_pdf(pdf_path, year):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    areas = [\"Linguagens\", \"Ciências Humanas\", \"Ciências da Natureza\", \"Matemática\"]\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Expressão regular para capturar os blocos de questões\n",
    "    question_blocks = re.split(r'Questão\\s+\\d+\\s*-\\s*', text)[1:]  # Dividindo por questão\n",
    "\n",
    "    questions = []\n",
    "    questions_full = []\n",
    "\n",
    "    for block in question_blocks:\n",
    "        lines = block.splitlines()\n",
    "\n",
    "        # Extraindo a área de conhecimento\n",
    "        area_conhecimento = next((word for word in areas if re.search(rf'\\b{word}\\b', lines[0])), None)\n",
    "\n",
    "        # Extraindo o enunciado até as alternativas e removendo a área de conhecimento do bloco de texto\n",
    "        enunciado = ' '.join(lines[1:-5])\n",
    "\n",
    "        # As alternativas foram consideradas como sendo as ultimas 5 linhas do bloco\n",
    "        if len(lines) >= 5: \n",
    "            questions.append({\n",
    "                \"Ano\": year,\n",
    "                \"Enunciado\": enunciado.strip(),\n",
    "                # Nas alternativas são removidas as letras \"A\" - \"E\" que estão no começo.\n",
    "                \"Alternativa_A\": lines[-5][1:].strip(), \n",
    "                \"Alternativa_B\": lines[-4][1:].strip(),\n",
    "                \"Alternativa_C\": lines[-3][1:].strip(),\n",
    "                \"Alternativa_D\": lines[-2][1:].strip(),\n",
    "                \"Alternativa_E\": lines[-1][1:].strip(),\n",
    "                \"Area_de_Conhecimento\": area_conhecimento\n",
    "            })\n",
    "\n",
    "        enunciado_alternativas = ' '.join(lines[1:])\n",
    "\n",
    "        questions_full.append({\n",
    "                \"Ano\": year,\n",
    "                \"Enunciado_Alternativas\": enunciado_alternativas.strip(),\n",
    "                \"Area_de_Conhecimento\": area_conhecimento\n",
    "            })\n",
    "\n",
    "    return questions, questions_full\n",
    "\n",
    "def save_to_csv(questions, csv_path):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    keys = questions[0].keys() if questions else []\n",
    "    \n",
    "    with open(csv_path, 'a', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        if not file_exists:\n",
    "            dict_writer.writeheader()\n",
    "        dict_writer.writerows(questions)\n",
    "\n",
    "def process_pdfs_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            questions, questions_full= extract_questions_from_pdf(pdf_path, filename[5:9])\n",
    "            save_to_csv(questions, 'CSV/alternativas_separadas.csv')\n",
    "            save_to_csv(questions_full, 'CSV/tudo_junto.csv')\n",
    "\n",
    "folder_path = 'Provas/'\n",
    "\n",
    "process_pdfs_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração das questões em TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_from_txt(txt_path, area_conhecimento, year):\n",
    "    with open(txt_path, 'r', encoding='cp1252') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Expressão regular para capturar os blocos de questões\n",
    "    question_blocks = re.split(r'QUESTÃO\\s+\\d+', text)[1:]  # Dividindo por questão\n",
    "    \n",
    "    questions = []\n",
    "    questions_full = []\n",
    "\n",
    "    for block in question_blocks:\n",
    "        lines = block.splitlines()\n",
    "\n",
    "        lines = lines if lines[-1] != '' else lines[:-1]\n",
    "\n",
    "        # Extraindo o enunciado e alternativas\n",
    "        enunciado = ' '.join(lines[1:-5]).strip()\n",
    "\n",
    "        # As alternativas são as últimas 5 linhas do bloco\n",
    "        if len(lines) >= 5:\n",
    "            questions.append({\n",
    "                \"Ano\": year,\n",
    "                \"Enunciado\": enunciado,\n",
    "                # Nas alternativas são removidas as letras \"a.\" - \"e.\" que estão no começo.\n",
    "                \"Alternativa_A\": lines[-5][2:].strip(),\n",
    "                \"Alternativa_B\": lines[-4][2:].strip(),\n",
    "                \"Alternativa_C\": lines[-3][2:].strip(),\n",
    "                \"Alternativa_D\": lines[-2][2:].strip(),\n",
    "                \"Alternativa_E\": lines[-1][2:].strip(),\n",
    "                \"Area_de_Conhecimento\": area_conhecimento\n",
    "            })\n",
    "\n",
    "        enunciado_alternativas = ' '.join(lines).strip()\n",
    "\n",
    "        questions_full.append({\n",
    "                \"Ano\": year,\n",
    "                \"Enunciado_Alternativas\": enunciado_alternativas.strip(),\n",
    "                \"Area_de_Conhecimento\": area_conhecimento\n",
    "            })\n",
    "    \n",
    "    return questions, questions_full\n",
    "\n",
    "def save_to_csv(questions, csv_path):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    keys = questions[0].keys() if questions else []\n",
    "    \n",
    "    with open(csv_path, 'a', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        if not file_exists:\n",
    "            dict_writer.writeheader()\n",
    "        dict_writer.writerows(questions)\n",
    "\n",
    "def process_txts_in_folder(folder_path):\n",
    "    area_map = {\n",
    "        \"CH\": \"Ciências Humanas\",\n",
    "        \"LC\": \"Linguagens\",\n",
    "        \"CN\": \"Ciências da Natureza\",\n",
    "        \"MT\": \"Matemática\"\n",
    "    }\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.txt'):\n",
    "            area_code = filename.split('_')[-1].replace('.txt', '')\n",
    "            area_conhecimento = area_map.get(area_code, \"Área desconhecida\")\n",
    "\n",
    "            txt_path = os.path.join(folder_path, filename)\n",
    "            questions, questions_full = extract_questions_from_txt(txt_path, area_conhecimento, filename[5:9])\n",
    "            save_to_csv(questions, 'CSV/alternativas_separadas.csv')\n",
    "            save_to_csv(questions_full, 'CSV/tudo_junto.csv')\n",
    "\n",
    "folder_path = 'Provas/'\n",
    "\n",
    "process_txts_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo linhas duplicadas e adicionando \"possível imagem\" nas células vazias\n",
    "As provas em PDF de Linguagens foram extraídas duas vezes por conta das questões de espanhol e inglês. Também, como foi feita apenas extração de texto, células vazias muito provavelmente significam imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_duplicatas(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.drop_duplicates()\n",
    "    df.fillna(\"possível imagem\", inplace=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "csv_path = 'CSV/alternativas_separadas.csv'\n",
    "remover_duplicatas(csv_path)\n",
    "\n",
    "csv_path = 'CSV/tudo_junto.csv'\n",
    "remover_duplicatas(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento de texto\n",
    "Aqui são aplicadas algumas técnicas de pré-processamento de texto, tais como deixar tudo minúsculo, remover pontuações, fazer tokenização, fazer Stemming, remover stopwords e palavras pequenas mantendo termos importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Converte todo o texto para letras minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove pontuação do texto\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Cria um tokenizador que usa expressões regulares para separar as palavras (tokens)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "    # RSLPStemmer é o stemmer para português\n",
    "    stemmer = RSLPStemmer()\n",
    "\n",
    "    # Lista de termos importantes (Linguagens, Ciências Humanas, Ciências da Natureza, Matemática)\n",
    "    termos_importantes = {\n",
    "        \"literatura\", \"gramática\", \"redação\", \"interpretação\", \"semântica\",\n",
    "        \"figuras\", \"coesão\", \"coerência\", \"gêneros\", \"sintaxe\", \"morfologia\",\n",
    "        \"poesia\", \"história\", \"geografia\", \"sociologia\", \"filosofia\", \"política\",\n",
    "        \"economia\", \"cultura\", \"revolução\", \"cidadania\", \"constituição\", \"guerra\",\n",
    "        \"democracia\", \"direitos\", \"biologia\", \"física\", \"química\", \"célula\",\n",
    "        \"genética\", \"evolução\", \"fotossíntese\", \"termoquímica\", \"tabela\", \"periodica\",\n",
    "        \"eletricidade\", \"força\", \"energia\", \"função\", \"álgebra\", \"geometria\",\n",
    "        \"probabilidade\", \"estatística\", \"cálculo\", \"triângulo\", \"progressão\", \"vetor\", \"polinômio\"\n",
    "    }\n",
    "\n",
    "    # Filtra os tokens para remover as stopwords, palavras curtas e aplica stemming\n",
    "    # Considerando que as questões de matemática podem conter só dígitos ou somente um dígitos, essas serão mantidas.\n",
    "    tokens = [\n",
    "        stemmer.stem(word) if (word not in termos_importantes and not word.isdigit()) else word \n",
    "        for word in tokens \n",
    "        if word not in stop_words and (len(word) > 2 or word in termos_importantes or word.isdigit())\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Para o DB com as alternativas separadas\n",
    "df = pd.read_csv('CSV/alternativas_separadas.csv')\n",
    "for c in df.columns[1:-1]:\n",
    "    df[c] = df[c].apply(preprocess_text)\n",
    "df.to_csv('CSV/alternativas_separadas_pp.csv', index=False)\n",
    "\n",
    "# Para o DB com o enunciado e alternativas juntos\n",
    "df = pd.read_csv('CSV/tudo_junto.csv')\n",
    "df['Enunciado_Alternativas'] = df['Enunciado_Alternativas'].apply(preprocess_text)\n",
    "df.to_csv('CSV/tudo_junto_pp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
